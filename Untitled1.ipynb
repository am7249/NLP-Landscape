{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('sample_text.txt')\n",
    "text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_preprocess(text):\n",
    "    # obtains tokens with a least 1 alphabet\n",
    "    pattern = re.compile(r'[A-Za-z]+[\\w^\\']*|[\\w^\\']*[A-Za-z]+[\\w^\\']*')\n",
    "    pattern = pattern.findall(text.lower())\n",
    "    pattern = [word for word in pattern if word not in stopwords.words('english')]\n",
    "    return pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapping(tokens):\n",
    "    word_to_id = dict()\n",
    "    id_to_word = dict()\n",
    "\n",
    "    for i, token in enumerate(set(tokens)):\n",
    "        word_to_id[token] = i\n",
    "        id_to_word[i] = token\n",
    "\n",
    "    return word_to_id, id_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data(tokens, word_to_id, window_size):\n",
    "    N = len(tokens)\n",
    "    X, Y = [], []\n",
    "\n",
    "    for i in range(N):\n",
    "        nbr_inds = list(range(max(0, i - window_size), i)) + \\\n",
    "                   list(range(i + 1, min(N, i + window_size + 1)))\n",
    "        for j in nbr_inds:\n",
    "            X.append(word_to_id[tokens[i]])\n",
    "            Y.append(word_to_id[tokens[j]])\n",
    "            \n",
    "    X = np.array(X)\n",
    "    X = np.expand_dims(X, axis=0)\n",
    "    Y = np.array(Y)\n",
    "    Y = np.expand_dims(Y, axis=0)\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenize_preprocess(text)\n",
    "word_to_id, id_to_word = mapping(tokens)\n",
    "X, Y = generate_training_data(tokens, word_to_id, 3)\n",
    "vocab_size = len(id_to_word)\n",
    "m = Y.shape[1]\n",
    "# turn Y into one hot encoding\n",
    "Y_one_hot = np.zeros((vocab_size, m))\n",
    "Y_one_hot[Y.flatten(), np.arange(m)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_wrd_emb(vocab_size, emb_size):\n",
    "    \"\"\"\n",
    "    vocab_size: int. vocabulary size of your corpus or training data\n",
    "    emb_size: int. word embedding size. How many dimensions to represent each vocabulary\n",
    "    \"\"\"\n",
    "    WRD_EMB = np.random.randn(vocab_size, emb_size) * 0.01\n",
    "    return WRD_EMB\n",
    "\n",
    "def initialize_dense(input_size, output_size):\n",
    "    \"\"\"\n",
    "    input_size: int. size of the input to the dense layer\n",
    "    output_szie: int. size of the output out of the dense layer\n",
    "    \"\"\"\n",
    "    W = np.random.randn(output_size, input_size) * 0.01\n",
    "    return W\n",
    "\n",
    "def initialize_parameters(vocab_size, emb_size):\n",
    "    \"\"\"\n",
    "    initialize all the trianing parameters\n",
    "    \"\"\"\n",
    "    WRD_EMB = initialize_wrd_emb(vocab_size, emb_size)\n",
    "    W = initialize_dense(emb_size, vocab_size)\n",
    "    \n",
    "    parameters = {}\n",
    "    parameters['WRD_EMB'] = WRD_EMB\n",
    "    parameters['W'] = W\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ind_to_word_vecs(inds, parameters):\n",
    "    \"\"\"\n",
    "    inds: numpy array. shape: (1, m)\n",
    "    parameters: dict. weights to be trained\n",
    "    \"\"\"\n",
    "    m = inds.shape[1] #number of training instances\n",
    "    WRD_EMB = parameters['WRD_EMB']\n",
    "    word_vec = WRD_EMB[inds.flatten(), :].T\n",
    "    \n",
    "    assert(word_vec.shape == (WRD_EMB.shape[1], m))\n",
    "    \n",
    "    return word_vec\n",
    "\n",
    "def linear_dense(word_vec, parameters):\n",
    "    \"\"\"\n",
    "    word_vec: numpy array. shape: (emb_size, m)\n",
    "    parameters: dict. weights to be trained\n",
    "    \"\"\"\n",
    "    m = word_vec.shape[1]\n",
    "    W = parameters['W']\n",
    "    Z = np.dot(W, word_vec)\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], m))\n",
    "    \n",
    "    return W, Z\n",
    "\n",
    "def softmax(Z):\n",
    "    \"\"\"\n",
    "    Z: output out of the dense layer. shape: (vocab_size, m)\n",
    "    \"\"\"\n",
    "    softmax_out = np.divide(np.exp(Z), np.sum(np.exp(Z), axis=0, keepdims=True) + 0.001)\n",
    "    \n",
    "    assert(softmax_out.shape == Z.shape)\n",
    "\n",
    "    return softmax_out\n",
    "\n",
    "def forward_propagation(inds, parameters):\n",
    "    word_vec = ind_to_word_vecs(inds, parameters)\n",
    "    W, Z = linear_dense(word_vec, parameters)\n",
    "    softmax_out = softmax(Z)\n",
    "    \n",
    "    caches = {}\n",
    "    caches['inds'] = inds\n",
    "    caches['word_vec'] = word_vec\n",
    "    caches['W'] = W\n",
    "    caches['Z'] = Z\n",
    "    \n",
    "    return softmax_out, caches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(softmax_out, Y):\n",
    "    \"\"\"\n",
    "    softmax_out: output out of softmax. shape: (vocab_size, m)\n",
    "    \"\"\"\n",
    "    m = softmax_out.shape[1]\n",
    "    cost = -(1 / m) * np.sum(np.sum(Y * np.log(softmax_out + 0.001), axis=0, keepdims=True), axis=1)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_backward(Y, softmax_out):\n",
    "    \"\"\"\n",
    "    Y: labels of training data. shape: (vocab_size, m)\n",
    "    softmax_out: output out of softmax. shape: (vocab_size, m)\n",
    "    \"\"\"\n",
    "    dL_dZ = softmax_out - Y\n",
    "    \n",
    "    assert(dL_dZ.shape == softmax_out.shape)\n",
    "    return dL_dZ\n",
    "\n",
    "def dense_backward(dL_dZ, caches):\n",
    "    \"\"\"\n",
    "    dL_dZ: shape: (vocab_size, m)\n",
    "    caches: dict. results from each steps of forward propagation\n",
    "    \"\"\"\n",
    "    W = caches['W']\n",
    "    word_vec = caches['word_vec']\n",
    "    m = word_vec.shape[1]\n",
    "    \n",
    "    dL_dW = (1 / m) * np.dot(dL_dZ, word_vec.T)\n",
    "    dL_dword_vec = np.dot(W.T, dL_dZ)\n",
    "\n",
    "    assert(W.shape == dL_dW.shape)\n",
    "    assert(word_vec.shape == dL_dword_vec.shape)\n",
    "    \n",
    "    return dL_dW, dL_dword_vec\n",
    "\n",
    "def backward_propagation(Y, softmax_out, caches):\n",
    "    dL_dZ = softmax_backward(Y, softmax_out)\n",
    "    dL_dW, dL_dword_vec = dense_backward(dL_dZ, caches)\n",
    "    \n",
    "    gradients = dict()\n",
    "    gradients['dL_dZ'] = dL_dZ\n",
    "    gradients['dL_dW'] = dL_dW\n",
    "    gradients['dL_dword_vec'] = dL_dword_vec\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "def update_parameters(parameters, caches, gradients, learning_rate):\n",
    "    vocab_size, emb_size = parameters['WRD_EMB'].shape\n",
    "    inds = caches['inds']\n",
    "    WRD_EMB = parameters['WRD_EMB']\n",
    "    dL_dword_vec = gradients['dL_dword_vec']\n",
    "    m = inds.shape[-1]\n",
    "    \n",
    "    WRD_EMB[inds.flatten(), :] -= dL_dword_vec.T * learning_rate\n",
    "\n",
    "    parameters['W'] -= learning_rate * gradients['dL_dW']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skipgram_model_training(X, Y, vocab_size, emb_size, learning_rate, epochs, batch_size=256, parameters=None, print_cost=True, plot_cost=True):\n",
    "    \"\"\"\n",
    "    X: Input word indices. shape: (1, m)\n",
    "    Y: One-hot encodeing of output word indices. shape: (vocab_size, m)\n",
    "    vocab_size: vocabulary size of your corpus or training data\n",
    "    emb_size: word embedding size. How many dimensions to represent each vocabulary\n",
    "    learning_rate: alaph in the weight update formula\n",
    "    epochs: how many epochs to train the model\n",
    "    batch_size: size of mini batch\n",
    "    parameters: pre-trained or pre-initialized parameters\n",
    "    print_cost: whether or not to print costs during the training process\n",
    "    \"\"\"\n",
    "    costs = []\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    if parameters is None:\n",
    "        parameters = initialize_parameters(vocab_size, emb_size)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_cost = 0\n",
    "        batch_inds = list(range(0, m, batch_size))\n",
    "        np.random.shuffle(batch_inds)\n",
    "        for i in batch_inds:\n",
    "            X_batch = X[:, i:i+batch_size]\n",
    "            Y_batch = Y[:, i:i+batch_size]\n",
    "\n",
    "            softmax_out, caches = forward_propagation(X_batch, parameters)\n",
    "            gradients = backward_propagation(Y_batch, softmax_out, caches)\n",
    "            update_parameters(parameters, caches, gradients, learning_rate)\n",
    "            cost = cross_entropy(softmax_out, Y_batch)\n",
    "            epoch_cost += np.squeeze(cost)\n",
    "            \n",
    "        costs.append(epoch_cost)\n",
    "        if print_cost and epoch % (epochs // 500) == 0:\n",
    "            print(\"Cost after epoch {}: {}\".format(epoch, epoch_cost))\n",
    "        if epoch % (epochs // 100) == 0:\n",
    "            learning_rate *= 0.98\n",
    "            \n",
    "    if plot_cost:\n",
    "        plt.plot(np.arange(epochs), costs)\n",
    "        plt.xlabel('# of epochs')\n",
    "        plt.ylabel('cost')\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 975.6291000980938\n",
      "Cost after epoch 1: 975.6262578595293\n",
      "Cost after epoch 2: 975.6234770945313\n",
      "Cost after epoch 3: 975.6206232119752\n",
      "Cost after epoch 4: 975.6177796900616\n",
      "Cost after epoch 5: 975.6148477894255\n",
      "Cost after epoch 6: 975.6118996060992\n",
      "Cost after epoch 7: 975.6088853799823\n",
      "Cost after epoch 8: 975.6058294293624\n",
      "Cost after epoch 9: 975.6027054250986\n",
      "Cost after epoch 10: 975.5994336263311\n",
      "Cost after epoch 11: 975.5960311454987\n",
      "Cost after epoch 12: 975.5926051569228\n",
      "Cost after epoch 13: 975.5889749420309\n",
      "Cost after epoch 14: 975.585168610735\n",
      "Cost after epoch 15: 975.5812280013556\n",
      "Cost after epoch 16: 975.5770614185493\n",
      "Cost after epoch 17: 975.5727472355228\n",
      "Cost after epoch 18: 975.5682459591857\n",
      "Cost after epoch 19: 975.5634351799681\n",
      "Cost after epoch 20: 975.5583550301568\n",
      "Cost after epoch 21: 975.5529922756866\n",
      "Cost after epoch 22: 975.5473511882319\n",
      "Cost after epoch 23: 975.541324759864\n",
      "Cost after epoch 24: 975.5349605023321\n",
      "Cost after epoch 25: 975.5281231072313\n",
      "Cost after epoch 26: 975.520774262435\n",
      "Cost after epoch 27: 975.513150101329\n",
      "Cost after epoch 28: 975.5049197499426\n",
      "Cost after epoch 29: 975.496052157908\n",
      "Cost after epoch 30: 975.4865018902398\n",
      "Cost after epoch 31: 975.476351335741\n",
      "Cost after epoch 32: 975.4654703133606\n",
      "Cost after epoch 33: 975.453713006225\n",
      "Cost after epoch 34: 975.4412498387421\n",
      "Cost after epoch 35: 975.4274875836077\n",
      "Cost after epoch 36: 975.4128593302552\n",
      "Cost after epoch 37: 975.3972325224959\n",
      "Cost after epoch 38: 975.3802418500859\n",
      "Cost after epoch 39: 975.3618626376706\n",
      "Cost after epoch 40: 975.3418526889137\n",
      "Cost after epoch 41: 975.3202937152395\n",
      "Cost after epoch 42: 975.2969542878944\n",
      "Cost after epoch 43: 975.2716505904388\n",
      "Cost after epoch 44: 975.2441844187927\n",
      "Cost after epoch 45: 975.2141547793465\n",
      "Cost after epoch 46: 975.1815162537546\n",
      "Cost after epoch 47: 975.1463991774679\n",
      "Cost after epoch 48: 975.1082257986197\n",
      "Cost after epoch 49: 975.066866724845\n",
      "Cost after epoch 50: 975.0202619340371\n",
      "Cost after epoch 51: 974.9705832096936\n",
      "Cost after epoch 52: 974.9170802981494\n",
      "Cost after epoch 53: 974.8573688094184\n",
      "Cost after epoch 54: 974.7930286319031\n",
      "Cost after epoch 55: 974.7205880853867\n",
      "Cost after epoch 56: 974.6430626857494\n",
      "Cost after epoch 57: 974.5582578759954\n",
      "Cost after epoch 58: 974.4645775063913\n",
      "Cost after epoch 59: 974.3633841135924\n",
      "Cost after epoch 60: 974.2507407391637\n",
      "Cost after epoch 61: 974.124836812613\n",
      "Cost after epoch 62: 973.991843475909\n",
      "Cost after epoch 63: 973.8412843092804\n",
      "Cost after epoch 64: 973.6791344482706\n",
      "Cost after epoch 65: 973.4999353830741\n",
      "Cost after epoch 66: 973.3018350205934\n",
      "Cost after epoch 67: 973.0896423382078\n",
      "Cost after epoch 68: 972.8577142415776\n",
      "Cost after epoch 69: 972.6038984559074\n",
      "Cost after epoch 70: 972.3224312126814\n",
      "Cost after epoch 71: 972.019644524354\n",
      "Cost after epoch 72: 971.7110716604067\n",
      "Cost after epoch 73: 971.3699552124045\n",
      "Cost after epoch 74: 971.0143316219785\n",
      "Cost after epoch 75: 970.650379336477\n",
      "Cost after epoch 76: 970.2618138932067\n",
      "Cost after epoch 77: 969.8629523268165\n",
      "Cost after epoch 78: 969.4569941522228\n",
      "Cost after epoch 79: 969.0173552511405\n",
      "Cost after epoch 80: 968.5763315374244\n",
      "Cost after epoch 81: 968.1200123585353\n",
      "Cost after epoch 82: 967.6476183113383\n",
      "Cost after epoch 83: 967.1628721767156\n",
      "Cost after epoch 84: 966.6574469267952\n",
      "Cost after epoch 85: 966.1473483885193\n",
      "Cost after epoch 86: 965.6120100607739\n",
      "Cost after epoch 87: 965.0672085849677\n",
      "Cost after epoch 88: 964.5422881729445\n",
      "Cost after epoch 89: 963.9808075221965\n",
      "Cost after epoch 90: 963.4163752369896\n",
      "Cost after epoch 91: 962.852320088494\n",
      "Cost after epoch 92: 962.2774874208054\n",
      "Cost after epoch 93: 961.7069488274719\n",
      "Cost after epoch 94: 961.1273680068093\n",
      "Cost after epoch 95: 960.538665217081\n",
      "Cost after epoch 96: 959.9616380147453\n",
      "Cost after epoch 97: 959.3757168139065\n",
      "Cost after epoch 98: 958.7910687203415\n",
      "Cost after epoch 99: 958.1954997904064\n",
      "Cost after epoch 100: 957.5923902766076\n",
      "Cost after epoch 101: 956.9984232147166\n",
      "Cost after epoch 102: 956.421356016433\n",
      "Cost after epoch 103: 955.7918410387172\n",
      "Cost after epoch 104: 955.231881250481\n",
      "Cost after epoch 105: 954.647099696545\n",
      "Cost after epoch 106: 954.0447747068562\n",
      "Cost after epoch 107: 953.4560205635331\n",
      "Cost after epoch 108: 952.8683595221986\n",
      "Cost after epoch 109: 952.2906573588871\n",
      "Cost after epoch 110: 951.7033585198141\n",
      "Cost after epoch 111: 951.0961786787519\n",
      "Cost after epoch 112: 950.5497085065143\n",
      "Cost after epoch 113: 949.9775270930934\n",
      "Cost after epoch 114: 949.3994460508828\n",
      "Cost after epoch 115: 948.8108804205995\n",
      "Cost after epoch 116: 948.221839572049\n",
      "Cost after epoch 117: 947.6513191451153\n",
      "Cost after epoch 118: 947.0890245639894\n",
      "Cost after epoch 119: 946.5089603272357\n",
      "Cost after epoch 120: 945.9895103671055\n",
      "Cost after epoch 121: 945.3966042767825\n",
      "Cost after epoch 122: 944.8524166435814\n",
      "Cost after epoch 123: 944.2782746142033\n",
      "Cost after epoch 124: 943.6977218579484\n",
      "Cost after epoch 125: 943.1764621263458\n",
      "Cost after epoch 126: 942.5974952289583\n",
      "Cost after epoch 127: 942.1076920850038\n",
      "Cost after epoch 128: 941.5707526242398\n",
      "Cost after epoch 129: 941.0212313761389\n",
      "Cost after epoch 130: 940.478478523993\n",
      "Cost after epoch 131: 939.9367793402178\n",
      "Cost after epoch 132: 939.409778501911\n",
      "Cost after epoch 133: 938.877899601777\n",
      "Cost after epoch 134: 938.3678670104387\n",
      "Cost after epoch 135: 937.8620032743047\n",
      "Cost after epoch 136: 937.3257825693664\n",
      "Cost after epoch 137: 936.7852865398246\n",
      "Cost after epoch 138: 936.3196559050272\n",
      "Cost after epoch 139: 935.7896757966503\n",
      "Cost after epoch 140: 935.2702369741612\n",
      "Cost after epoch 141: 934.7409352338824\n",
      "Cost after epoch 142: 934.2489642449159\n",
      "Cost after epoch 143: 933.739187827589\n",
      "Cost after epoch 144: 933.2619496922202\n",
      "Cost after epoch 145: 932.7697301065107\n",
      "Cost after epoch 146: 932.2128772658099\n",
      "Cost after epoch 147: 931.7211990665844\n",
      "Cost after epoch 148: 931.2383514996021\n",
      "Cost after epoch 149: 930.7605237617846\n",
      "Cost after epoch 150: 930.2736179882473\n",
      "Cost after epoch 151: 929.7220220670664\n",
      "Cost after epoch 152: 929.2788940901605\n",
      "Cost after epoch 153: 928.7811027017264\n",
      "Cost after epoch 154: 928.2681625723317\n",
      "Cost after epoch 155: 927.7824198436042\n",
      "Cost after epoch 156: 927.2955923490698\n",
      "Cost after epoch 157: 926.8154918748152\n",
      "Cost after epoch 158: 926.3285977936589\n",
      "Cost after epoch 159: 925.8500743836586\n",
      "Cost after epoch 160: 925.3665675135644\n",
      "Cost after epoch 161: 924.8467892761815\n",
      "Cost after epoch 162: 924.3866639033904\n",
      "Cost after epoch 163: 923.9046404131661\n",
      "Cost after epoch 164: 923.4358770470776\n",
      "Cost after epoch 165: 922.9891440559607\n",
      "Cost after epoch 166: 922.4740684180206\n",
      "Cost after epoch 167: 921.9877145111084\n",
      "Cost after epoch 168: 921.5517058815536\n",
      "Cost after epoch 169: 921.0385639636602\n",
      "Cost after epoch 170: 920.5924842931198\n",
      "Cost after epoch 171: 920.1200476653252\n",
      "Cost after epoch 172: 919.6476306039202\n",
      "Cost after epoch 173: 919.1545610797593\n",
      "Cost after epoch 174: 918.7330714909343\n",
      "Cost after epoch 175: 918.2389518403921\n",
      "Cost after epoch 176: 917.8172104093724\n",
      "Cost after epoch 177: 917.2951611701476\n",
      "Cost after epoch 178: 916.8326373540343\n",
      "Cost after epoch 179: 916.4006638705155\n",
      "Cost after epoch 180: 915.9134650164676\n",
      "Cost after epoch 181: 915.479339156571\n",
      "Cost after epoch 182: 915.0077602268474\n",
      "Cost after epoch 183: 914.5364325688203\n",
      "Cost after epoch 184: 914.0722947872529\n",
      "Cost after epoch 185: 913.6020894229108\n",
      "Cost after epoch 186: 913.1700078594183\n",
      "Cost after epoch 187: 912.7103405360504\n",
      "Cost after epoch 188: 912.2644957356626\n",
      "Cost after epoch 189: 911.7941406607888\n",
      "Cost after epoch 190: 911.3558172007316\n",
      "Cost after epoch 191: 910.9181366405428\n",
      "Cost after epoch 192: 910.4131316400955\n",
      "Cost after epoch 193: 910.004658647483\n",
      "Cost after epoch 194: 909.5611885887832\n",
      "Cost after epoch 195: 909.0945415839661\n",
      "Cost after epoch 196: 908.6515449828704\n",
      "Cost after epoch 197: 908.1711158770996\n",
      "Cost after epoch 198: 907.7717042443533\n",
      "Cost after epoch 199: 907.3108919951313\n",
      "Cost after epoch 200: 906.86135913578\n",
      "Cost after epoch 201: 906.4374015629877\n",
      "Cost after epoch 202: 905.9913149007326\n",
      "Cost after epoch 203: 905.5298624965266\n",
      "Cost after epoch 204: 905.1439829770818\n",
      "Cost after epoch 205: 904.6765459888397\n",
      "Cost after epoch 206: 904.2713355929741\n",
      "Cost after epoch 207: 903.8540512807848\n",
      "Cost after epoch 208: 903.35961745848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 209: 902.9197131698885\n",
      "Cost after epoch 210: 902.529558615554\n",
      "Cost after epoch 211: 902.1348230233788\n",
      "Cost after epoch 212: 901.6654002276048\n",
      "Cost after epoch 213: 901.2201963944112\n",
      "Cost after epoch 214: 900.8160991460081\n",
      "Cost after epoch 215: 900.3836921879596\n",
      "Cost after epoch 216: 899.9743392111305\n",
      "Cost after epoch 217: 899.5674475817364\n",
      "Cost after epoch 218: 899.0980677532064\n",
      "Cost after epoch 219: 898.7004762221701\n",
      "Cost after epoch 220: 898.2319912159663\n",
      "Cost after epoch 221: 897.8645931624434\n",
      "Cost after epoch 222: 897.4255221545012\n",
      "Cost after epoch 223: 897.0311080127298\n",
      "Cost after epoch 224: 896.5955855438523\n",
      "Cost after epoch 225: 896.2016760201271\n",
      "Cost after epoch 226: 895.7952020743503\n",
      "Cost after epoch 227: 895.3322547606049\n",
      "Cost after epoch 228: 894.94645784552\n",
      "Cost after epoch 229: 894.5618851852098\n",
      "Cost after epoch 230: 894.1027064799695\n",
      "Cost after epoch 231: 893.7401975042086\n",
      "Cost after epoch 232: 893.3269543737356\n",
      "Cost after epoch 233: 892.9308792151396\n",
      "Cost after epoch 234: 892.5251670225568\n",
      "Cost after epoch 235: 892.1144607937405\n",
      "Cost after epoch 236: 891.7081020560042\n",
      "Cost after epoch 237: 891.3167580336651\n",
      "Cost after epoch 238: 890.9384842231816\n",
      "Cost after epoch 239: 890.516200422969\n",
      "Cost after epoch 240: 890.1060044830258\n",
      "Cost after epoch 241: 889.746348614478\n",
      "Cost after epoch 242: 889.3380762102869\n",
      "Cost after epoch 243: 888.9260454049368\n",
      "Cost after epoch 244: 888.5544083274729\n",
      "Cost after epoch 245: 888.1564353256357\n",
      "Cost after epoch 246: 887.7746068327237\n",
      "Cost after epoch 247: 887.3718373850688\n",
      "Cost after epoch 248: 886.9896406203277\n",
      "Cost after epoch 249: 886.590290300902\n",
      "Cost after epoch 250: 886.210685597364\n",
      "Cost after epoch 251: 885.8213202030234\n",
      "Cost after epoch 252: 885.4391233285558\n",
      "Cost after epoch 253: 885.0704102898569\n",
      "Cost after epoch 254: 884.7070611978315\n",
      "Cost after epoch 255: 884.3104061940736\n",
      "Cost after epoch 256: 883.920782392936\n",
      "Cost after epoch 257: 883.5714298400433\n",
      "Cost after epoch 258: 883.1903249693255\n",
      "Cost after epoch 259: 882.7925134930477\n",
      "Cost after epoch 260: 882.4298312069303\n",
      "Cost after epoch 261: 882.0565081640102\n",
      "Cost after epoch 262: 881.6714204190175\n",
      "Cost after epoch 263: 881.2996974925325\n",
      "Cost after epoch 264: 880.931935622071\n",
      "Cost after epoch 265: 880.5690215513996\n",
      "Cost after epoch 266: 880.1965962191257\n",
      "Cost after epoch 267: 879.841578580027\n",
      "Cost after epoch 268: 879.453208259495\n",
      "Cost after epoch 269: 879.1075897040146\n",
      "Cost after epoch 270: 878.7452765810247\n",
      "Cost after epoch 271: 878.3809521105646\n",
      "Cost after epoch 272: 878.0061185190224\n",
      "Cost after epoch 273: 877.6609024680528\n",
      "Cost after epoch 274: 877.3093699655027\n",
      "Cost after epoch 275: 876.9504144018595\n",
      "Cost after epoch 276: 876.5696078939459\n",
      "Cost after epoch 277: 876.2090690093828\n",
      "Cost after epoch 278: 875.8793038133306\n",
      "Cost after epoch 279: 875.4981043054717\n",
      "Cost after epoch 280: 875.1739061778611\n",
      "Cost after epoch 281: 874.7722641768775\n",
      "Cost after epoch 282: 874.4315375627647\n",
      "Cost after epoch 283: 874.1053482927799\n",
      "Cost after epoch 284: 873.7577485498487\n",
      "Cost after epoch 285: 873.3826771028123\n",
      "Cost after epoch 286: 873.0191474550766\n",
      "Cost after epoch 287: 872.707439338525\n",
      "Cost after epoch 288: 872.3463892445618\n",
      "Cost after epoch 289: 872.0427501568554\n",
      "Cost after epoch 290: 871.6779976532783\n",
      "Cost after epoch 291: 871.3152298928137\n",
      "Cost after epoch 292: 870.9890935792603\n",
      "Cost after epoch 293: 870.6566093276401\n",
      "Cost after epoch 294: 870.3160751981482\n",
      "Cost after epoch 295: 869.9791451274994\n",
      "Cost after epoch 296: 869.620526777831\n",
      "Cost after epoch 297: 869.2866940063641\n",
      "Cost after epoch 298: 868.961876599577\n",
      "Cost after epoch 299: 868.61943320014\n",
      "Cost after epoch 300: 868.2681263615875\n",
      "Cost after epoch 301: 867.9632332265021\n",
      "Cost after epoch 302: 867.6259489430346\n",
      "Cost after epoch 303: 867.2725015362755\n",
      "Cost after epoch 304: 866.9883425007755\n",
      "Cost after epoch 305: 866.6237904610504\n",
      "Cost after epoch 306: 866.3199388507493\n",
      "Cost after epoch 307: 865.9592283991667\n",
      "Cost after epoch 308: 865.6482473345842\n",
      "Cost after epoch 309: 865.3301206910531\n",
      "Cost after epoch 310: 865.0191536102085\n",
      "Cost after epoch 311: 864.7004958046184\n",
      "Cost after epoch 312: 864.3434964539358\n",
      "Cost after epoch 313: 864.0594304389984\n",
      "Cost after epoch 314: 863.7237205762298\n",
      "Cost after epoch 315: 863.3983804150687\n",
      "Cost after epoch 316: 863.105598388096\n",
      "Cost after epoch 317: 862.775658348748\n",
      "Cost after epoch 318: 862.4578991670587\n",
      "Cost after epoch 319: 862.1524543578869\n",
      "Cost after epoch 320: 861.8367356647922\n",
      "Cost after epoch 321: 861.516838851696\n",
      "Cost after epoch 322: 861.1746629592982\n",
      "Cost after epoch 323: 860.8784053589812\n",
      "Cost after epoch 324: 860.6134287928377\n",
      "Cost after epoch 325: 860.2801787740815\n",
      "Cost after epoch 326: 859.9820039616111\n",
      "Cost after epoch 327: 859.6354504602576\n",
      "Cost after epoch 328: 859.3542295611538\n",
      "Cost after epoch 329: 859.0531511727858\n",
      "Cost after epoch 330: 858.7776100709531\n",
      "Cost after epoch 331: 858.4414988045581\n",
      "Cost after epoch 332: 858.1618491491276\n",
      "Cost after epoch 333: 857.8614650882128\n",
      "Cost after epoch 334: 857.5415396400068\n",
      "Cost after epoch 335: 857.2602005931872\n",
      "Cost after epoch 336: 856.9444102339615\n",
      "Cost after epoch 337: 856.6664306046611\n",
      "Cost after epoch 338: 856.3660337142484\n",
      "Cost after epoch 339: 856.0733318403529\n",
      "Cost after epoch 340: 855.7822779072826\n",
      "Cost after epoch 341: 855.4764148538163\n",
      "Cost after epoch 342: 855.1770227417971\n",
      "Cost after epoch 343: 854.9002431772981\n",
      "Cost after epoch 344: 854.6074466982152\n",
      "Cost after epoch 345: 854.3333685060661\n",
      "Cost after epoch 346: 854.0364407824733\n",
      "Cost after epoch 347: 853.7645098659813\n",
      "Cost after epoch 348: 853.4518512106403\n",
      "Cost after epoch 349: 853.177652049813\n",
      "Cost after epoch 350: 852.9015111283335\n",
      "Cost after epoch 351: 852.6037369545127\n",
      "Cost after epoch 352: 852.3362821451985\n",
      "Cost after epoch 353: 852.0608576122186\n",
      "Cost after epoch 354: 851.8065204988603\n",
      "Cost after epoch 355: 851.5009945732398\n",
      "Cost after epoch 356: 851.2270562951837\n",
      "Cost after epoch 357: 850.9482215574404\n",
      "Cost after epoch 358: 850.6687301888587\n",
      "Cost after epoch 359: 850.4176199313375\n",
      "Cost after epoch 360: 850.1274522993206\n",
      "Cost after epoch 361: 849.8487771508602\n",
      "Cost after epoch 362: 849.5596480680576\n",
      "Cost after epoch 363: 849.2992237597316\n",
      "Cost after epoch 364: 849.0388065311017\n",
      "Cost after epoch 365: 848.768442527726\n",
      "Cost after epoch 366: 848.511242836923\n",
      "Cost after epoch 367: 848.2320902511896\n",
      "Cost after epoch 368: 847.9710012539839\n",
      "Cost after epoch 369: 847.6959557955023\n",
      "Cost after epoch 370: 847.4278443765263\n",
      "Cost after epoch 371: 847.1758171617151\n",
      "Cost after epoch 372: 846.9176164562821\n",
      "Cost after epoch 373: 846.6495771254434\n",
      "Cost after epoch 374: 846.3997689177434\n",
      "Cost after epoch 375: 846.1309825449991\n",
      "Cost after epoch 376: 845.8728863890744\n",
      "Cost after epoch 377: 845.6019498706154\n",
      "Cost after epoch 378: 845.3623852401768\n",
      "Cost after epoch 379: 845.0948212688004\n",
      "Cost after epoch 380: 844.8454731641036\n",
      "Cost after epoch 381: 844.577292312662\n",
      "Cost after epoch 382: 844.3402599356837\n",
      "Cost after epoch 383: 844.097409985908\n",
      "Cost after epoch 384: 843.8257826312309\n",
      "Cost after epoch 385: 843.572138632565\n",
      "Cost after epoch 386: 843.3319923094468\n",
      "Cost after epoch 387: 843.0778871675841\n",
      "Cost after epoch 388: 842.8435426528256\n",
      "Cost after epoch 389: 842.6061654859026\n",
      "Cost after epoch 390: 842.3300010168125\n",
      "Cost after epoch 391: 842.0914910973343\n",
      "Cost after epoch 392: 841.856981289436\n",
      "Cost after epoch 393: 841.6088899959975\n",
      "Cost after epoch 394: 841.3790261096473\n",
      "Cost after epoch 395: 841.1116512671476\n",
      "Cost after epoch 396: 840.8854905987943\n",
      "Cost after epoch 397: 840.6366024079418\n",
      "Cost after epoch 398: 840.4076690269931\n",
      "Cost after epoch 399: 840.1634818010918\n",
      "Cost after epoch 400: 839.9244604119474\n",
      "Cost after epoch 401: 839.671424396485\n",
      "Cost after epoch 402: 839.4543927175841\n",
      "Cost after epoch 403: 839.2297327080033\n",
      "Cost after epoch 404: 839.0042001379136\n",
      "Cost after epoch 405: 838.7528576717202\n",
      "Cost after epoch 406: 838.505287002414\n",
      "Cost after epoch 407: 838.2926833354484\n",
      "Cost after epoch 408: 838.0613169180278\n",
      "Cost after epoch 409: 837.8288109780456\n",
      "Cost after epoch 410: 837.6023924623127\n",
      "Cost after epoch 411: 837.3671664723532\n",
      "Cost after epoch 412: 837.1362790096354\n",
      "Cost after epoch 413: 836.9193160181406\n",
      "Cost after epoch 414: 836.6830655251946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 415: 836.457986092375\n",
      "Cost after epoch 416: 836.223490909961\n",
      "Cost after epoch 417: 836.005184996492\n",
      "Cost after epoch 418: 835.7976272488995\n",
      "Cost after epoch 419: 835.5757199331665\n",
      "Cost after epoch 420: 835.3518539132149\n",
      "Cost after epoch 421: 835.143761133926\n",
      "Cost after epoch 422: 834.9126399019879\n",
      "Cost after epoch 423: 834.6777825682782\n",
      "Cost after epoch 424: 834.4734585548897\n",
      "Cost after epoch 425: 834.2465366511202\n",
      "Cost after epoch 426: 834.0357623475724\n",
      "Cost after epoch 427: 833.8186914454405\n",
      "Cost after epoch 428: 833.6173416629024\n",
      "Cost after epoch 429: 833.3967218250694\n",
      "Cost after epoch 430: 833.1862048056298\n",
      "Cost after epoch 431: 832.9653428276131\n",
      "Cost after epoch 432: 832.7495434424463\n",
      "Cost after epoch 433: 832.5581678827364\n",
      "Cost after epoch 434: 832.3449700564191\n",
      "Cost after epoch 435: 832.124998273018\n",
      "Cost after epoch 436: 831.9180465854271\n",
      "Cost after epoch 437: 831.7103056032723\n",
      "Cost after epoch 438: 831.5059021759421\n",
      "Cost after epoch 439: 831.2939867721686\n",
      "Cost after epoch 440: 831.0969700624736\n",
      "Cost after epoch 441: 830.885481475806\n",
      "Cost after epoch 442: 830.6835935522599\n",
      "Cost after epoch 443: 830.4735953236006\n",
      "Cost after epoch 444: 830.2812857135104\n",
      "Cost after epoch 445: 830.078207053591\n",
      "Cost after epoch 446: 829.8657817393868\n",
      "Cost after epoch 447: 829.670755524836\n",
      "Cost after epoch 448: 829.4668837209797\n",
      "Cost after epoch 449: 829.2737347063907\n",
      "Cost after epoch 450: 829.0794245654942\n",
      "Cost after epoch 451: 828.8785812575279\n",
      "Cost after epoch 452: 828.6797867600473\n",
      "Cost after epoch 453: 828.488774401029\n",
      "Cost after epoch 454: 828.3032981994802\n",
      "Cost after epoch 455: 828.1116715553101\n",
      "Cost after epoch 456: 827.9023248773016\n",
      "Cost after epoch 457: 827.7114488880085\n",
      "Cost after epoch 458: 827.5313355966651\n",
      "Cost after epoch 459: 827.3274527467128\n",
      "Cost after epoch 460: 827.1493811622596\n",
      "Cost after epoch 461: 826.9374840203574\n",
      "Cost after epoch 462: 826.7555756612762\n",
      "Cost after epoch 463: 826.5781613680415\n",
      "Cost after epoch 464: 826.3784411539259\n",
      "Cost after epoch 465: 826.2102652667076\n",
      "Cost after epoch 466: 826.0075189099035\n",
      "Cost after epoch 467: 825.8274141702716\n",
      "Cost after epoch 468: 825.6229364488787\n",
      "Cost after epoch 469: 825.4564412260225\n",
      "Cost after epoch 470: 825.2856659835861\n",
      "Cost after epoch 471: 825.0832646822046\n",
      "Cost after epoch 472: 824.9099337646825\n",
      "Cost after epoch 473: 824.7178877238586\n",
      "Cost after epoch 474: 824.5569280557653\n",
      "Cost after epoch 475: 824.369521177164\n",
      "Cost after epoch 476: 824.1835252063946\n",
      "Cost after epoch 477: 824.0133723371383\n",
      "Cost after epoch 478: 823.8354976103334\n",
      "Cost after epoch 479: 823.6407802479204\n",
      "Cost after epoch 480: 823.4811076059125\n",
      "Cost after epoch 481: 823.2951586149068\n",
      "Cost after epoch 482: 823.1340726576075\n",
      "Cost after epoch 483: 822.9392401318679\n",
      "Cost after epoch 484: 822.7678946341276\n",
      "Cost after epoch 485: 822.5981453782747\n",
      "Cost after epoch 486: 822.4229054680184\n",
      "Cost after epoch 487: 822.2676886113026\n",
      "Cost after epoch 488: 822.0830154357734\n",
      "Cost after epoch 489: 821.9226210540234\n",
      "Cost after epoch 490: 821.747067989381\n",
      "Cost after epoch 491: 821.5636458703377\n",
      "Cost after epoch 492: 821.4171698940077\n",
      "Cost after epoch 493: 821.241866907021\n",
      "Cost after epoch 494: 821.0718479396579\n",
      "Cost after epoch 495: 820.9094983776503\n",
      "Cost after epoch 496: 820.7334167824904\n",
      "Cost after epoch 497: 820.5617196520732\n",
      "Cost after epoch 498: 820.4005765639607\n",
      "Cost after epoch 499: 820.2410371292793\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "paras = skipgram_model_training(X, Y_one_hot, vocab_size, 50, 0.05, 500,\\\n",
    "                                batch_size=128, parameters=None, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type a word: pakistan\n"
     ]
    }
   ],
   "source": [
    "X_test = input(\"Type a word: \")\n",
    "if X_test in word_to_id:\n",
    "    X_test = [word_to_id[X_test]]\n",
    "else:\n",
    "    raise Exception(\"no such word\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test = np.array([32])\n",
    "X_test = np.expand_dims(X_test, axis=0)\n",
    "softmax_test, _ = forward_propagation(X_test, paras)\n",
    "top_sorted_inds = np.argsort(softmax_test, axis=0)[-4:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word:  pakistan\n",
      "closest words:  india  with id:  1027\n",
      "closest words:  movement  with id:  543\n",
      "closest words:  history  with id:  667\n",
      "closest words:  pakistan  with id:  1349\n"
     ]
    }
   ],
   "source": [
    "print(\"word: \", id_to_word[X_test[0][0]])\n",
    "vectors_closest = []\n",
    "for i in top_sorted_inds:\n",
    "    print(\"closest words: \",id_to_word[i[0]], \" with id: \", i[0])\n",
    "    vectors_closest.append(paras['WRD_EMB'][i[0],:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-41.64859360836507"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(vectors_closest[0], paras['WRD_EMB'][X_test[0][0],:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.2253427263739259\n"
     ]
    }
   ],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "a, b = vectors_closest[0], paras['WRD_EMB'][X_test[0][0],:]\n",
    "cos_sim = dot(a, b)/(norm(a)*norm(b))\n",
    "print(cos_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 32 is out of bounds for axis 1 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-83d1ce582621>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#for input_ind in range(10):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0minput_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mid_to_word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0moutput_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mid_to_word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moutput_ind\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0moutput_ind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtop_sorted_inds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}'s neighbor words: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 32 is out of bounds for axis 1 with size 1"
     ]
    }
   ],
   "source": [
    "#for input_ind in range(10):\n",
    "input_word = id_to_word[32]\n",
    "output_words = [id_to_word[output_ind] for output_ind in top_sorted_inds[::-1, 32]]\n",
    "print(\"{}'s neighbor words: {}\".format(input_word, output_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fasttext\n",
    "\n",
    "import fasttext\n",
    "\n",
    "# Skipgram model :\n",
    "#model = fasttext.train_unsupervised('sample_text.txt', model='skipgram')\n",
    "\n",
    "# or, cbow model :\n",
    "model = fasttext.train_unsupervised('sample_text.txt', model='cbow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.9645183682441711, 'Kuomintang'),\n",
       " (0.9639371037483215, 'through'),\n",
       " (0.9637499451637268, 'British'),\n",
       " (0.9630894660949707, 'replaced'),\n",
       " (0.9623491168022156, 'during'),\n",
       " (0.9621705412864685, 'Qing'),\n",
       " (0.962152361869812, 'Empire,'),\n",
       " (0.9620386362075806, 'East'),\n",
       " (0.9619960784912109, 'Muslim'),\n",
       " (0.9619114398956299, 'by')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_nearest_neighbors('xinjiang')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
